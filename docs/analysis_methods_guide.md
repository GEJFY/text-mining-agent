# NexusText AI v7.0 分析手法ガイド

> **対象読者**: テキストマイニングやデータ分析の経験がない方
> **前提知識**: 特になし（専門用語はすべて解説します）
> **最終更新**: 2026年2月23日

---

## はじめに：テキストマイニングとは

### テキストマイニングを一言でいうと

テキストマイニングとは、**大量の文章データから有用な情報やパターンを自動的に見つけ出す技術**です。

日常の例で考えてみましょう。あなたの会社に毎月1万件のお客様アンケートが届くとします。
これを1件ずつ読んで内容をまとめるには、膨大な時間がかかります。テキストマイニングは、
この作業をコンピュータの力で自動化し、数分で全体の傾向や重要なポイントを把握できるようにします。

```
従来の方法:
  お客様の声 1万件 → 人が1件ずつ読む → 数週間かかる → まとめを作成

テキストマイニング:
  お客様の声 1万件 → NexusText AI → 数分 → 自動で傾向・パターンを発見
```

### なぜビジネスで重要なのか

現代のビジネスでは、メール、アンケート、SNS投稿、社内報告書、コールセンターの通話記録など、
日々膨大な量のテキストデータが生まれています。このデータには、次のような「宝の山」が眠っています。

- **お客様が本当に求めていること**（表面的な要望の裏にある本質的なニーズ）
- **問題が大きくなる前の兆候**（クレームの増加傾向、特定の不満の集中）
- **競合他社に対する自社の強み・弱み**
- **社内の不正やコンプライアンス違反の兆候**

テキストマイニングは、これらを**人間の目では見落としがちなパターン**として浮かび上がらせます。

### NexusText AI v7.0 でできること - 実際の活用シーン

| 活用シーン | 具体例 | 使う機能 |
|---|---|---|
| 顧客フィードバック分析 | 「最近、配送に関する不満が急増している」を発見 | 感情分析 + 時系列分析 |
| サポート品質改善 | 「問い合わせの40%が同じ操作方法の質問」と判明 | クラスタリング分析 |
| リスク検知 | 社内通報データから不正の兆候を早期発見 | リスク感情分析 + エージェント |
| 市場調査 | SNS投稿から「価格」と「品質」の関連性を可視化 | 共起ネットワーク分析 |
| 監査レポート | 大量の監査所見を自動分類・要約 | 自律型AIエージェント |

---

## 第1章：テキストの前処理（データをきれいにする）

### 1.1 なぜ前処理が必要か

**たとえ話：料理の下ごしらえ**

テキスト分析の前処理は、料理でいう「下ごしらえ」にあたります。

おいしいカレーを作るとき、野菜を洗わずに泥つきのまま鍋に入れたりしませんよね。
皮をむき、適切なサイズに切り、必要のない部分は取り除きます。

テキストデータも同じです。生のデータには、分析の邪魔になる「泥」がたくさんついています。

```
生のテキストデータの例:

  "<p>ＣＳ対応が　とても悪かったです。。。</p>"

問題点:
  ├── <p>...</p>  → HTMLタグ（Webページの書式情報）
  ├── ＣＳ        → 全角英字（半角の "CS" と別物として扱われる）
  ├── 　          → 全角スペース（余計な空白）
  └── 。。。      → 不要な繰り返し記号
```

前処理をしないと、コンピュータは「CS」と「ＣＳ」を別の言葉だと認識してしまいます。
人間なら同じだとわかりますが、コンピュータには明示的に教える必要があるのです。

**NexusText AI の前処理パイプライン（処理の流れ）:**

```
入力テキスト
    │
    ▼
[1] HTML/マークアップ除去     ← Webから取得したデータのタグを除去
    │
    ▼
[2] 文字コード正規化（NFKC）  ← 全角・半角を統一
    │
    ▼
[3] トークナイゼーション      ← 文章を単語に分割
    │
    ▼
[4] ストップワード除去        ← 「は」「の」「が」などを除去
    │
    ▼
[5] Embedding生成            ← 単語・文章を数値ベクトルに変換
    │
    ▼
きれいなデータ（分析可能な状態）
```

### 1.2 トークナイゼーション（文章を単語に分ける）

#### トークナイゼーションとは

トークナイゼーション（トークン化）とは、**文章を意味のある最小単位（単語）に分割する処理**です。

英語の場合、単語はスペースで区切られているので比較的簡単です。

```
英語の場合:
  "I love this product" → ["I", "love", "this", "product"]
  （スペースで区切るだけ）
```

しかし、日本語には単語の間にスペースがありません。これが日本語のテキスト処理を
特別なものにしている大きな理由です。

```
日本語の場合:
  "このサービスは素晴らしいです" → ???

  人間なら: 「この」「サービス」「は」「素晴らしい」「です」
  単純な分割では不可能！
```

#### 日本語のトークナイゼーション - MeCab/fugashi

NexusText AI では、**MeCab（メカブ）** という形態素解析エンジンを使います。
（内部的には fugashi という Python ライブラリを通じて MeCab を利用しています。）

形態素解析とは、文章を文法的に正しい最小単位（形態素）に分解する技術です。
日本語の「辞書」と「文法ルール」を使って、最も自然な分割を見つけます。

```
入力: "顧客対応の改善が必要です"

MeCab の解析結果:
  顧客     名詞,一般           ← 分析で使う（重要な意味を持つ）
  対応     名詞,サ変接続        ← 分析で使う
  の       助詞,連体化          ← ストップワード（後で除去）
  改善     名詞,サ変接続        ← 分析で使う
  が       助詞,格助詞          ← ストップワード
  必要     名詞,形容動詞語幹    ← 分析で使う
  です     助動詞              ← ストップワード
```

NexusText AI では、**名詞・動詞・形容詞**のみを抽出します。
これらが文章の「意味」を運ぶ重要な単語だからです。

#### 実際の日本語テキストでの例

```
元のテキスト:
  "先月購入した商品の品質に問題があり、交換を希望します"

トークナイゼーション後:
  ["先月", "購入", "商品", "品質", "問題", "交換", "希望"]

→ 助詞や助動詞が除去され、意味のある単語だけが残りました。
  この文が「品質の問題」「交換の希望」について述べていることがわかります。
```

```
元のテキスト:
  "担当者の説明がわかりにくく、何度も問い合わせが必要だった"

トークナイゼーション後:
  ["担当者", "説明", "問い合わせ", "必要"]

→ 「わかりにくい」は形容詞として、「何度」は副詞として
  処理されます。名詞を中心に抽出することで、
  主要なトピック（担当者、説明、問い合わせ）が明確になります。
```

#### 英語との比較

| 特徴 | 日本語 | 英語 |
|---|---|---|
| 単語の区切り | なし（形態素解析が必要） | スペースで区切られている |
| 大文字/小文字 | なし | あり（"Apple" と "apple" の統一が必要） |
| 活用形の処理 | 複雑（「食べる」「食べた」「食べない」） | 比較的単純（"eat", "ate", "eaten"） |
| 必要なツール | MeCab, Juman++ 等の専用ツール | 比較的簡単なルールで対応可能 |

### 1.3 ストップワード除去

#### ストップワードとは

ストップワードとは、**文章の中で頻繁に出現するが、分析上あまり意味を持たない単語**のことです。

たとえ話で説明します。友人に「昨日何をした？」と聞かれたとき、あなたは
「映画を見に行った」と答えます。このとき、重要な情報は「映画」「見る」「行く」です。
「を」「に」は文法的に必要ですが、内容の理解には直接関係しません。

#### NexusText AI のストップワードリスト

**日本語ストップワード（プリセット）:**

```
助詞:   の、に、は、を、が、で、て、と
動詞:   ある、いる、する
名詞:   こと、これ、それ、もの
形容詞: ない
助動詞: です、ます
その他:  し、れ、さ、た
```

**英語ストップワード（プリセット）:**

```
冠詞:     the, a, an
Be動詞:   is, are, was, were, be, been, being
助動詞:   have, has, had, do, does, did, will, would, could, should
前置詞:   in, on, at, to, for, of, with, by, from
接続詞:   and, or, but, if
代名詞:   this, that
```

#### なぜ除去が必要なのか

ストップワードを除去しないと、分析結果がノイズだらけになります。

```
除去前の単語頻度ランキング:
  1位: の（5,234回）    ← 意味のない単語が上位を占める
  2位: は（4,891回）
  3位: を（3,456回）
  4位: が（3,102回）
  5位: で（2,890回）
  ...
  15位: 対応（987回）   ← 本当に知りたい情報が埋もれる
  16位: 品質（945回）

除去後の単語頻度ランキング:
  1位: 対応（987回）    ← 重要な単語が上位に浮上
  2位: 品質（945回）
  3位: 改善（823回）
  4位: 問い合わせ（756回）
  5位: 担当者（634回）
```

#### カスタムストップワード

NexusText AI では、プリセットのストップワードに加えて、
分析対象に応じた**カスタムストップワード**を追加できます。

例えば、自社の社名やサービス名など、すべてのテキストに含まれるが
分析上の価値が低い単語を追加するとよいでしょう。

```python
# カスタムストップワードの追加例
custom_stopwords = {"弊社", "株式会社", "お客様", "サービス"}
```

### 1.4 正規化（NFKC正規化）

#### 正規化とは

正規化とは、**見た目が違うけれど同じ意味の文字を統一する処理**です。

日本語のテキストには、同じ文字でも全角・半角の違いがあります。
コンピュータはこれらを完全に別の文字として扱うため、統一が必要です。

```
正規化前:                    正規化後:
  Ｈｅｌｌｏ              → Hello        （全角英数字 → 半角）
  ０１２３                 → 0123         （全角数字 → 半角）
  ﾃｽﾄ                    → テスト        （半角カタカナ → 全角）
  ㈱                      → (株)         （特殊記号の展開）
  ﬁ                       → fi           （合字の分解）
```

#### NFKC正規化を選ぶ理由

Unicode（文字コードの国際規格）には4種類の正規化形式がありますが、
NexusText AI では **NFKC** を採用しています。

```
正規化の種類:
  NFC   - 文字を合成する（基本的な正規化）
  NFD   - 文字を分解する
  NFKC  - 互換文字も統一 + 合成  ← NexusText AI はこれを使用
  NFKD  - 互換文字も統一 + 分解

NexusText AI が NFKC を選ぶ理由:
  ├── 全角・半角の統一ができる
  ├── 特殊記号の展開ができる
  ├── 日本語テキスト処理の業界標準
  └── MeCab との相性が良い
```

#### 実際の効果

```
正規化前:
  "ＣＳ対応が　とても良かった！！ＴＨＡＮＫＳ"

正規化後:
  "CS対応が とても良かった!!THANKS"

効果:
  - 「ＣＳ」と「CS」が同じ単語として認識される
  - 全角スペースが半角スペースに統一される
  - 分析の精度が大幅に向上する
```

---

## 第2章：クラスタリング分析（似た文章をグループにまとめる）

### 2.1 クラスタリングとは

#### たとえ話：洗濯物の仕分け

クラスタリングは、**似ているものを自動的にグループに分ける技術**です。

洗濯物を仕分ける場面を想像してください。
あなたは自然と、シャツはシャツ、靴下は靴下、タオルはタオルと分けますよね。
色別に分けることもあるでしょう。白い物、濃い色の物、デリケートな素材の物...

クラスタリングは、コンピュータが同じことをテキストデータに対して行います。
何千件ものコメントを、内容の似たもの同士で自動的にグループ化します。

```
お客様の声 3,000件

  クラスタリング実行
       │
       ▼
  ┌─────────────────────────────────────────────────────────┐
  │                                                         │
  │  クラスター1         クラスター2         クラスター3      │
  │  「配送の遅延」      「商品の品質」       「CS対応」       │
  │  (450件)            (380件)             (520件)          │
  │                                                         │
  │  クラスター4         クラスター5                          │
  │  「価格への不満」    「ポイント制度」                      │
  │  (290件)            (210件)                              │
  │                                                         │
  │  ※ 残り1,150件は他のクラスターに分類                     │
  └─────────────────────────────────────────────────────────┘
```

### 2.2 Embeddingベクトル（文章を数値に変換する）

#### なぜ数値に変換するのか

コンピュータは文章をそのままでは比較できません。
「この2つの文章は似ている」と判断するには、文章を**数値の並び（ベクトル）**に変換する必要があります。

この変換を **Embedding（エンベディング）** と呼びます。

#### たとえ話：住所を地図上の座標に変換する

Embedding は、「住所を地図上の座標（緯度・経度）に変換する」ことに似ています。

```
住所（テキスト）:
  "東京都千代田区丸の内1-1"  → 座標 (35.6812, 139.7671)
  "東京都千代田区大手町1-1"  → 座標 (35.6858, 139.7634)
  "大阪府大阪市北区梅田1-1"  → 座標 (34.7024, 135.4959)

座標同士の距離:
  丸の内 ↔ 大手町 = 近い（同じ千代田区）
  丸の内 ↔ 梅田   = 遠い（東京と大阪）

同じように、テキストのEmbedding:
  "商品の品質が悪い"    → [0.23, -0.15, 0.87, ...] (384次元)
  "製品のクオリティ不足" → [0.21, -0.18, 0.85, ...] (384次元)
  "配送が遅い"         → [-0.45, 0.67, 0.12, ...] (384次元)

ベクトル間の距離:
  品質が悪い ↔ クオリティ不足 = 近い（似た意味）
  品質が悪い ↔ 配送が遅い     = 遠い（違うトピック）
```

#### Sentence-BERT（センテンスバート）とは

NexusText AI では、**Sentence-BERT** というモデルを使って Embedding を生成します。
具体的には `paraphrase-multilingual-MiniLM-L12-v2` というモデルを使用しています。

このモデルの特徴:

| 特徴 | 説明 |
|---|---|
| 多言語対応 | 日本語・英語・中国語など50以上の言語に対応 |
| 文章レベルの理解 | 単語ではなく文章全体の意味を捉える |
| 384次元 | 各テキストを384個の数値で表現する |
| 高速処理 | MiniLM ベースで軽量かつ高精度 |

```
Sentence-BERT の動作イメージ:

  "カスタマーサポートの対応が素晴らしい"
        │
        ▼
  ┌─────────────────────────┐
  │   Sentence-BERT モデル    │
  │                           │
  │  - 文脈を理解              │
  │  - 単語の関係性を把握      │
  │  - 意味を数値化            │
  └─────────────────────────┘
        │
        ▼
  [0.12, -0.34, 0.56, 0.78, ..., -0.23]  ← 384個の数値
```

### 2.3 UMAP次元削減（高次元を2次元に圧縮する）

#### なぜ次元削減が必要か

Embedding は384次元のベクトルですが、人間が理解できるのは2次元（平面）か3次元（立体）です。
384次元のデータをそのままグラフに表示することはできません。

#### たとえ話：地球儀を世界地図にする

**UMAP（ユーマップ）** による次元削減は、「地球儀を平面の世界地図にする」ことに似ています。

```
地球儀（3次元）→ 世界地図（2次元）

  - 近い国同士は地図上でも近い位置に配置される
  - 遠い国同士は地図上でも離れた位置になる
  - 完全に正確ではないが、大まかな位置関係は保たれる

384次元のEmbedding → 2次元の散布図

  - 似た内容のテキストは近くに配置される
  - 違う内容のテキストは離れた位置になる
  - 人間が視覚的にクラスターの分布を確認できる
```

#### UMAPのパラメータ

NexusText AI では、UMAPに2つの重要なパラメータがあります。

**n_neighbors（エヌ・ネイバーズ）: デフォルト = 15**

「近所の何件を参考にするか」を決めるパラメータです。

```
n_neighbors が小さい場合（例: 5）:
  ├── 局所的な構造を重視
  ├── 小さなグループが多く見える
  └── 細かい違いが強調される

n_neighbors が大きい場合（例: 50）:
  ├── 全体的な構造を重視
  ├── 大きなまとまりが見える
  └── 細かい違いが無視される

たとえ話:
  n_neighbors=5  → 近所5軒だけ見て「この地域は住宅街」と判断
  n_neighbors=50 → 近所50軒を見て「この地域は住宅街とお店が混在」と判断
```

**min_dist（ミン・ディスト）: デフォルト = 0.1**

「点と点の最小距離」を決めるパラメータです。

```
min_dist が小さい場合（例: 0.0）:
  ├── 点が密集してかたまる
  ├── クラスターの境界がはっきりする
  └── 重なりが見えにくい

min_dist が大きい場合（例: 0.5）:
  ├── 点が広がって配置される
  ├── クラスター間のグラデーションが見える
  └── 全体の分布がわかりやすい
```

### 2.4 K-Means法（ケイミーンズ法）

#### K-Means法とは

K-Means法は、最も基本的で直感的なクラスタリング手法です。
**「k個のグループに分ける」** ことを目標にします。

#### 動作の仕組み - ステップバイステップ

```
【ステップ1】 k個の「中心点」をランダムに配置する

  散布図上のイメージ（k=3の場合）:

      .  .          .
    .  ★    .    .
      .  .       .     .
                    .
     .    .     ★     .
       .          .
    .       .        .
         .      .  ★
      .     .       .

  ★ = ランダムに配置された3つの中心点
  . = データ点（各テキストの位置）


【ステップ2】 各データ点を最も近い中心点のグループに割り当てる

      A  A          B
    A  ★A   B    B
      A  A       B     B
                    B
     A    A     ★B    B
       A          B
    C       C        C
         C      C  ★C
      C     C       C

  A, B, C = それぞれ最も近い中心点のグループに色分け


【ステップ3】 各グループの「重心」（平均位置）を新しい中心点にする

      A  A          B
    A   ★    B    B        ★の位置がグループの
      A  A       B     B   平均位置に移動
                    B
     A    A      ★    B
       C          B
    C       C        C
         C      C  ★
      C     C       C


【ステップ4】 ステップ2-3を中心点が動かなくなるまで繰り返す

  通常5-10回の繰り返しで安定します。
```

#### いつ使うのか

K-Means法は以下の場合に適しています:

- クラスター数をあらかじめ決められる場合
- 各クラスターが大体同じサイズの場合
- 球状（丸い形）のクラスターが期待できる場合
- 処理速度を重視する場合

#### k（クラスター数）の選び方

k の値を決めるのは、K-Means法の最大の課題です。NexusText AI では以下の方法を使います。

**エルボー法:**

```
k=2からk=10まで試し、「分散の減少具合」をグラフにします。
急激に減少しなくなる点（肘のように曲がる点）が最適なkです。

  分散
  │
  │＼
  │  ＼
  │    ＼
  │      ＼___________    ← この「肘」のあたりが最適
  │                       （この例ではk=4付近）
  └──────────────────→ k
    2   3   4   5   6   7
```

**シルエットスコア（後述）との組み合わせ:**

複数のkで試行し、シルエットスコアが最も高いkを選択します。

### 2.5 HDBSCAN（エイチディービースキャン）

#### 密度ベースクラスタリングとは

HDBSCANは、K-Meansとは全く異なるアプローチのクラスタリング手法です。
**データが密集している領域**を自動的にクラスターとして検出します。

#### たとえ話：夜空の星座

夜空を見上げると、星がたくさん見えます。星が密集しているところ（星座）と、
星がまばらなところがありますよね。HDBSCANは、この「星が密集している領域」を
自動的に見つけます。

```
K-Means の場合:
  「星空を3つの領域に分けなさい」→ 無理やり3つに分ける

HDBSCAN の場合:
  「星が密集しているところを見つけなさい」→ 自然なグループを発見

      ●●●                    ●
    ●●●●●●               ● ●
     ●●●●                    ●
                   ●
        ●        ●              ← この孤立した点は
                          ●       「外れ値」として検出
     ●●●●●
    ●●●●●●●●
     ●●●●●●
      ●●●●
```

#### HDBSCANの利点

| 特徴 | K-Means | HDBSCAN |
|---|---|---|
| クラスター数の指定 | 必要（kを事前に決める） | 不要（自動で決まる） |
| 外れ値の検出 | できない | できる（-1ラベル） |
| クラスターの形 | 球状のみ | 任意の形に対応 |
| 不均等なサイズ | 苦手 | 得意 |

NexusText AI のHDBSCAN設定:

```
min_cluster_size = 10  （最小10件でクラスターとみなす）
metric = "euclidean"   （ユークリッド距離を使用）
```

#### 外れ値（アウトライヤー）の検出

HDBSCANの大きな特徴は、**どのクラスターにも属さないデータ（外れ値）** を
自動的に検出できることです。これらのデータには `-1` というラベルが付きます。

外れ値は、以下のような重要な発見につながることがあります:

- 通常とは異なる異常なフィードバック
- 新しいトピックの兆候
- データ入力のミス

### 2.6 GMM（ガウス混合モデル）

#### 確率的アプローチとは

GMM（Gaussian Mixture Model）は、**各データ点が各クラスターに「どのくらいの確率で」属するか**
を計算する手法です。

#### たとえ話：ハーフタイムの選手

サッカーの試合で、あるスタジアムに2チームの応援団がいるとします。
K-Meansは「この人はAチームのファン」と断定しますが、
GMMは「この人はAチームのファンである確率が70%、Bチームのファンである確率が30%」と表現します。

```
K-Meansの場合:
  テキスト: "配送は遅いが商品は良い"
  結果: クラスター2（配送問題）に分類  ← 1つだけに割り当て

GMMの場合:
  テキスト: "配送は遅いが商品は良い"
  結果: クラスター1（商品品質）: 35%
        クラスター2（配送問題）: 55%
        クラスター3（CS対応）:  10%
  → 複数のクラスターにまたがることがわかる
```

#### いつGMMを使うか

- テキストが複数のトピックにまたがる場合
- クラスター間の境界が曖昧な場合
- 「この文章がどのグループに何%属するか」を知りたい場合

### 2.7 シルエットスコア（良いクラスタリングの判断基準）

#### シルエットスコアとは

シルエットスコアは、**クラスタリングの品質を数値で評価する指標**です。
-1.0 から 1.0 の範囲の値をとります。

```
シルエットスコアの解釈:

  +1.0  ← 完璧な分離（各クラスターが明確に分かれている）
   ...
  +0.7  ← 良好（強い構造がある）
   ...
  +0.5  ← まずまず（合理的な構造がある）
   ...
  +0.25 ← 弱い（構造があいまい）
   ...
   0.0  ← 構造なし（ランダムに分けたのと変わらない）
   ...
  -1.0  ← 最悪（分類が完全に間違っている）
```

#### 計算の考え方

```
各データ点について:
  a = 同じクラスター内の他の点との平均距離（凝集度）
  b = 最も近い他のクラスターの点との平均距離（分離度）

  シルエット値 = (b - a) / max(a, b)

直感的な理解:
  ├── 自分のグループ内で近い（aが小さい）      → 良い
  ├── 他のグループから遠い（bが大きい）        → 良い
  └── 結果: スコアが1に近いほど良いクラスタリング
```

NexusText AI では、外れ値（ラベルが -1 のデータ）を除外してからシルエットスコアを計算します。

### 2.8 実践ガイド：パラメータの選び方

以下のフローチャートを参考にしてください。

```
【クラスタリング手法の選択】

  クラスター数を事前に決められる？
      │
      ├── はい → クラスター間に重なりがある？
      │            │
      │            ├── はい → GMM を使う
      │            │           パラメータ: n_components = 想定クラスター数
      │            │
      │            └── いいえ → K-Means を使う
      │                         パラメータ: n_clusters = 想定クラスター数
      │
      └── いいえ → HDBSCAN を使う
                    パラメータ: min_cluster_size = データ量の1-5%


【UMAPパラメータの目安】

  データ量        n_neighbors    min_dist
  ─────────────────────────────────────
  ~500件          5-10           0.1
  500-5,000件     10-20          0.1
  5,000-50,000件  15-30          0.05-0.1
  50,000件~       30-50          0.05


【シルエットスコアの判断基準】

  0.7以上   → そのまま使える（素晴らしい結果）
  0.5-0.7   → 良好（ほとんどの場面で十分）
  0.3-0.5   → 可（パラメータ調整を検討）
  0.3未満   → 改善が必要（手法やパラメータの変更を推奨）
```

---

## 第3章：感情分析（テキストの感情を読み取る）

### 3.1 感情分析とは

#### 「ポジティブ/ネガティブ」を超えて

感情分析は、テキストに含まれる感情や意図を自動的に判定する技術です。

単純な「良い/悪い」の二分法ではなく、NexusText AI では**ビジネスの文脈に即した
多軸の分析**を提供します。

```
従来の感情分析:
  "対応が遅い" → ネガティブ  （これだけでは改善につながらない）

NexusText AI の感情分析:
  "対応が遅い" → 不満（CS対応に関する苦情）
                  信頼度: 0.92
                  根拠: 「対応が遅い」
                  → 具体的なアクション（CS部門への改善指示）に直結
```

### 3.2 LLMによる分類

NexusText AI では、**大規模言語モデル（LLM）** を使ってテキストを分類します。
従来の辞書ベースの感情分析と異なり、文脈を理解した高精度な判定が可能です。

```
辞書ベースの限界:
  "期待していたのに残念" → 「期待」=ポジティブ、「残念」=ネガティブ → 判定困難

LLMベースの強み:
  "期待していたのに残念" → 文脈を理解し「不満」と正しく判定
```

#### 処理の流れ

```
テキスト群（10件ずつバッチ処理）
    │
    ▼
┌───────────────────────────────────┐
│  LLM（タスクに応じて自動選択）      │
│                                     │
│  ラベリング/要約  → Claude Opus     │
│  バッチ分類      → Claude Sonnet   │
│  PII検知        → GPT-5-mini      │
│  機密データ     → Llama（ローカル） │
└───────────────────────────────────┘
    │
    ▼
各テキストに対する出力:
  ├── ラベル（分類結果）
  ├── スコア（0.0 ~ 1.0 の信頼度）
  └── エビデンス（判定の根拠となるテキスト箇所）
```

### 3.3 プリセット分析軸

NexusText AI には、3つのプリセット分析軸が用意されています。
用途に応じて選択します。

#### Basic（基本）モード

最もシンプルな3分類です。初めての方はこれから始めるのがおすすめです。

| 分類 | 説明 | テキスト例 |
|---|---|---|
| Positive | 肯定的な内容 | 「とても使いやすくて気に入っています」 |
| Negative | 否定的な内容 | 「品質が悪すぎて返品したい」 |
| Neutral | 中立的な内容 | 「商品を受け取りました。確認します」 |

#### Business（ビジネス）モード

顧客の声（VoC）分析に最適化された5分類です。

| 分類 | 説明 | テキスト例 |
|---|---|---|
| 満足 | 製品やサービスに対する満足 | 「新機能がとても便利で助かっています」 |
| 不満 | 不満や苦情 | 「3回も修理に出したのにまだ壊れる」 |
| 要望 | 改善要望や提案 | 「メール通知機能があると嬉しいです」 |
| 質問 | 問い合わせや確認 | 「この商品はMacでも使えますか？」 |
| その他 | 上記に該当しない | 「昨日店舗に行きました」 |

**Businessモードの活用例（デモデータより）:**

```
テキスト: "毎月の利用明細がわかりにくいので、もっとシンプルにしてほしい"
  → 分類: 要望
  → スコア: {要望: 0.88, 不満: 0.45}
  → 根拠: 「もっとシンプルにしてほしい」
  → アクション: 利用明細のUI改善を検討

テキスト: "先日の電話対応、丁寧で安心しました。ありがとうございます"
  → 分類: 満足
  → スコア: {満足: 0.95}
  → 根拠: 「丁寧で安心しました」
  → アクション: 好事例として共有、担当者を評価
```

#### Risk（リスク）モード

内部監査やコンプライアンス分野向けの4分類です。

| 分類 | 説明 | テキスト例 |
|---|---|---|
| コンプライアンスリスク | 法令・規則違反の可能性 | 「この手続きは法律上問題があるのでは」 |
| 不正兆候 | 不正行為の兆候 | 「領収書の日付と出張日が合わない」 |
| 統制不備 | 内部統制の不備 | 「承認なしで大口取引が処理されている」 |
| 改善要望 | 業務改善の提案 | 「チェック体制を見直した方がよい」 |

**Riskモードの活用例（デモデータより）:**

```
テキスト: "上長の承認を得ずに契約変更が行われたケースが複数見つかった"
  → 分類: 統制不備
  → スコア: {統制不備: 0.91, 不正兆候: 0.35}
  → 根拠: 「上長の承認を得ずに契約変更」
  → アクション: 承認フローの点検、該当契約の遡及確認

テキスト: "取引先からのキックバックの噂がある"
  → 分類: 不正兆候
  → スコア: {不正兆候: 0.87, コンプライアンスリスク: 0.72}
  → 根拠: 「キックバックの噂」
  → アクション: 内部調査の開始を検討
```

### 3.4 カスタム分析軸

プリセットでは対応できない独自の分析が必要な場合、**カスタム分析軸**を定義できます。

```python
# カスタム軸の定義例（従業員エンゲージメント調査）
custom_axes = [
    {"name": "やりがい",     "description": "仕事へのやりがい・達成感"},
    {"name": "人間関係",     "description": "職場の人間関係・チームワーク"},
    {"name": "成長機会",     "description": "スキルアップ・キャリア開発"},
    {"name": "待遇・福利厚生", "description": "給与・福利厚生への満足度"},
    {"name": "経営方針",     "description": "会社の方向性・ビジョンへの共感"},
]
```

カスタム軸を定義する際のポイント:

1. **軸同士が重ならないようにする** - 明確に区別できる定義を心がける
2. **具体的な説明を書く** - LLM が正確に判定するための手がかりになる
3. **5-7軸程度に抑える** - 多すぎると精度が下がる
4. **「その他」カテゴリを含める** - どこにも当てはまらないテキストの受け皿

### 3.5 マルチラベル分析

#### 1つのテキストが複数のカテゴリに属する場合

実際のテキストは、1つのカテゴリにきれいに収まらないことがよくあります。

```
シングルラベル（通常モード）:
  "配送は遅かったけど、商品自体は素晴らしかった"
  → 分類: 不満  （1つだけ選ぶ。しかし実際は満足の要素もある）

マルチラベル（multi_label=true）:
  "配送は遅かったけど、商品自体は素晴らしかった"
  → 分類: [不満, 満足]
  → スコア: {不満: 0.75, 満足: 0.82}
  → 根拠:
      不満 ← 「配送は遅かった」
      満足 ← 「商品自体は素晴らしかった」
```

マルチラベル分析は、テキストの**多面的な側面**を捉えたい場合に有効です。

### 3.6 時系列分析とスパイク検知

#### 時系列分析とは

感情分析の結果を**時間軸**で追跡することで、トレンドや変化を把握できます。

```
月別の感情分布の推移:

        Positive  Negative  Neutral
2025/10    45%       25%       30%
2025/11    42%       28%       30%
2025/12    38%       35%       27%    ← Negative が増加傾向
2026/01    30%       42%       28%    ← 明らかな変化
2026/02    35%       38%       27%

→ 2025/12 から Negative が急増。何が起きた？
  → 原因調査のトリガーとして活用
```

#### スパイク検知

スパイクとは、**通常の変動範囲を超えた急激な変化**のことです。
NexusText AI は、移動平均を使って自動的にスパイクを検知します。

```
スパイク検知の仕組み:

  件数
  │
  │              ●          ← スパイク（急激な増加）
  │             /│＼
  │  移動平均  / │  ＼
  │ ──●──●──  │   ──●──●──
  │         ＼  │  /
  │          ＼│/
  │
  └────────────────────→ 時間
    10月 11月 12月 1月 2月

検知ロジック:
  1. 直近 window 期間（デフォルト3）の移動平均を計算
  2. 実際の値と移動平均の差（偏差）を計算
  3. 偏差 > threshold x 標準偏差 の場合、スパイクと判定

パラメータ:
  window    = 3    （直近3期間の平均を使用）
  threshold = 2.0  （標準偏差の2倍を超えたらスパイク）
```

### 3.7 エビデンスハイライト

#### なぜ根拠が重要なのか

AIの分析結果を信頼するためには、**「なぜそう判断したのか」の根拠**が不可欠です。
NexusText AI は、すべての分類結果に対して、判定の根拠となるテキスト箇所をハイライトします。

```
テキスト:
  "新しいダッシュボード機能はとても見やすくなりましたが、
   レポートの出力に時間がかかりすぎます。改善をお願いします。"

分析結果:
  ├── ラベル: [満足, 不満, 要望]
  │
  ├── エビデンス:
  │   ├── 満足 ← 「ダッシュボード機能はとても見やすくなりました」
  │   ├── 不満 ← 「レポートの出力に時間がかかりすぎます」
  │   └── 要望 ← 「改善をお願いします」
  │
  └── スコア: {満足: 0.78, 不満: 0.85, 要望: 0.72}
```

エビデンスハイライトにより:

- 分析結果の**検証**が容易になる
- 誤分類を**発見**しやすくなる
- レポートに**具体的な引用**を含められる
- AIの**ハルシネーション（幻覚）** を抑制できる

---

## 第4章：共起ネットワーク分析（言葉のつながりを可視化する）

### 4.1 共起とは

#### たとえ話：いつも一緒にいる友達

「共起」とは、**2つの単語が同じ文章（またはその近く）に一緒に現れること**です。

学校で、AさんとBさんがいつも一緒にいるのを見かけたら、「この2人は仲が良いんだな」
と思いますよね。同じように、「品質」と「不良」という単語がよく一緒に出現するなら、
この2つには強い関連性があります。

```
テキストデータ:
  1. "商品の品質に問題がある"        → 品質 - 問題
  2. "品質改善を望みます"            → 品質 - 改善
  3. "配送の遅延が問題です"          → 配送 - 遅延 - 問題
  4. "品質は良いが配送が遅い"        → 品質 - 配送 - 遅い
  5. "品質管理を強化してほしい"      → 品質 - 管理 - 強化

共起の回数:
  品質 - 問題:  2回  ← よく一緒に出現する
  品質 - 改善:  1回
  品質 - 配送:  1回
  配送 - 遅延:  1回
  配送 - 問題:  1回
```

### 4.2 共起行列の構築

#### ウィンドウベースの共起カウント

NexusText AI では、**ウィンドウ**（窓）という概念を使って共起を数えます。
ウィンドウサイズとは、「何単語以内に一緒に出現したら共起とみなすか」の範囲です。

```
ウィンドウサイズ = 5 の場合:

  トークン列: [品質, 管理, 体制, 見直し, 必要, 早急, 対応, 求める]
                ├──────ウィンドウ──────┤
                  品質を起点に5単語以内の単語と共起をカウント

  品質-管理:  共起あり（1単語離れている）
  品質-体制:  共起あり（2単語離れている）
  品質-見直し: 共起あり（3単語離れている）
  品質-必要:  共起あり（4単語離れている）
  品質-早急:  共起なし（5単語以上離れている）
```

#### 実際のテキストでの例

```
入力テキスト:
  "カスタマーサポートの電話対応が丁寧で、問題がすぐに解決しました"

トークナイゼーション後:
  [カスタマーサポート, 電話, 対応, 丁寧, 問題, 解決]

ウィンドウサイズ=5 での共起ペア:
  (カスタマーサポート, 電話)    重複除去後の
  (カスタマーサポート, 対応)    ユニークペアで
  (カスタマーサポート, 丁寧)    カウント
  (カスタマーサポート, 問題)
  (電話, 対応)
  (電話, 丁寧)
  (電話, 問題)
  (電話, 解決)
  (対応, 丁寧)
  (対応, 問題)
  (対応, 解決)
  (丁寧, 問題)
  (丁寧, 解決)
  (問題, 解決)
```

#### 閾値フィルタリング

共起の回数が少ないペア（たまたま一緒に出現しただけ）はノイズになるため、
**最低出現回数（min_frequency）** でフィルタリングします。

```
パラメータ: min_frequency = 3（デフォルト）

フィルタリング前:              フィルタリング後:
  品質-問題:  15回     →       品質-問題:  15回  ✓
  品質-改善:   8回     →       品質-改善:   8回  ✓
  品質-配送:   5回     →       品質-配送:   5回  ✓
  品質-色:     2回     →       （除外）         ✗
  品質-天気:   1回     →       （除外）         ✗
```

### 4.3 ネットワークグラフの読み方

#### グラフの要素

共起ネットワークは、**ノード（点）** と **エッジ（線）** で構成されます。

```
  ネットワークグラフの読み方:

          配送 ─────── 遅延
         ╱    ╲         │
       ╱        ╲       │
    品質 ──── 問題 ──── 対応
      │ ╲              ╱
      │   ╲          ╱
    改善    不良   サポート


  ノード（丸い点）:
    ├── 各単語を表す
    ├── 大きさ = その単語の出現頻度（大きいほど頻出）
    └── 色 = 所属するコミュニティ（後述）

  エッジ（線）:
    ├── 2つの単語が共起することを表す
    ├── 太さ = 共起の頻度（太いほど頻繁に共起）
    └── 線がある = 関連性が強い
```

#### 読み取りのポイント

```
1. 大きなノード = 頻出単語
   → 全体のキートピックを把握

2. 太いエッジ = 強い共起関係
   → 密接に関連するトピックのペアを発見

3. ハブ（多くの線が集中するノード）
   → 中心的なトピック（複数のテーマに関連）

4. 孤立したグループ
   → 独立したトピック群
```

### 4.4 中心性指標

#### 中心性とは

ネットワーク内で、**どの単語が重要な位置にいるか** を数値化したものが中心性指標です。
NexusText AI では2種類の中心性を計算します。

#### 次数中心性（Degree Centrality）

**「どれだけ多くの単語とつながっているか」** を表します。

```
たとえ話：人気者

  クラスで友達が多い人 = 次数中心性が高い

  ネットワーク:
        A
       ╱│╲
      B  C  D        ← A は 5つの単語とつながっている
     ╱       ╲          → 次数中心性が最も高い
    E         F

  次数中心性:
    A: 5/5 = 1.0  ← 最も多くの単語と共起
    B: 2/5 = 0.4
    C: 1/5 = 0.2
    D: 2/5 = 0.4
    E: 1/5 = 0.2
    F: 1/5 = 0.2
```

**ビジネスでの意味**: 次数中心性が高い単語は、多くのトピックに関連する
**キーワード**です。例えば「対応」という単語が高ければ、
「電話対応」「クレーム対応」「配送対応」など、様々な文脈で使われていることを意味します。

#### 媒介中心性（Betweenness Centrality）

**「異なるグループをつなぐ橋渡し役になっているか」** を表します。

```
たとえ話：空港のハブ

  東京 ──── 大阪          東京 ──── ★福岡 ──── 沖縄
  東京 ──── 名古屋        大阪 ──── ★福岡 ──── 鹿児島

  福岡は、九州と本州をつなぐ「ハブ空港」の役割。
  福岡がなくなると、九州へのアクセスが大幅に悪化する。
  → 媒介中心性が高い

  ネットワーク:
    [品質グループ]      [配送グループ]
     品質 ── 改善       遅延 ── 配送
      │                  │
      └──── 対応 ────────┘      ← 「対応」が2つのグループを橋渡し
                                    → 媒介中心性が高い
```

**ビジネスでの意味**: 媒介中心性が高い単語は、**異なるトピック群を結ぶキーワード**です。
この単語を取り除くと、ネットワークが分断されます。
例えば「対応」が高ければ、品質問題と配送問題の両方に「対応」が関係していることを示します。

### 4.5 コミュニティ検出（Louvain法）

#### コミュニティとは

ネットワーク内で、互いに密接につながっている単語のグループを**コミュニティ**と呼びます。
これは、テキストデータに含まれる**主要なトピック群**に対応します。

#### Louvain法（ルーヴァン法）

NexusText AI では、**Louvain法**というアルゴリズムでコミュニティを検出します。

```
Louvain法の動作イメージ:

ステップ1: 各単語を個別のコミュニティとする
  {品質} {改善} {管理} {配送} {遅延} {対応} {サポート} {電話}

ステップ2: 隣接する単語を同じコミュニティに統合（モジュラリティが改善する場合のみ）
  {品質, 改善, 管理} {配送, 遅延} {対応, サポート, 電話}

ステップ3: さらに統合を試みる（改善しなくなるまで繰り返す）
  最終結果:
    コミュニティ1: {品質, 改善, 管理, 不良}     ← 品質管理の話題
    コミュニティ2: {配送, 遅延, 到着, 追跡}     ← 配送の話題
    コミュニティ3: {対応, サポート, 電話, 丁寧}   ← CS対応の話題
```

#### 可視化

コミュニティは通常、ネットワークグラフ上で**色分け**して表示されます。

```
  ■ 赤 = コミュニティ1（品質関連）
  ■ 青 = コミュニティ2（配送関連）
  ■ 緑 = コミュニティ3（CS対応関連）

       [赤]品質 ──── [赤]改善
         │   ╲
         │     [赤]不良
         │
       [緑]対応 ──── [緑]電話
         │
       [青]配送 ──── [青]遅延 ──── [青]追跡
```

### 4.6 モジュラリティスコア

#### モジュラリティとは

モジュラリティは、**コミュニティ分割の品質**を測る指標です。
0.0 から 1.0 の範囲の値をとります。

```
モジュラリティスコアの解釈:

  0.0     ← コミュニティ構造がない（ランダムなネットワーク）
  ...
  0.3     ← 弱いコミュニティ構造
  ...
  0.5     ← 中程度のコミュニティ構造
  ...
  0.7以上 ← 強いコミュニティ構造（明確なトピック群がある）
  ...
  1.0     ← 完全に分離されたコミュニティ
```

一般的に、0.3以上であれば意味のあるコミュニティ構造が存在するとされています。

### 4.7 時間スライスアニメーション

#### トピックの変遷を追跡する

NexusText AI では、データに日付情報がある場合、**時間スライス**による
共起ネットワークの変遷を分析できます。

```
設定パラメータ:
  time_interval = "month"  （月別にスライス）
  ※ "week"（週別）、"day"（日別）も選択可能

処理の流れ:
  全データ → 月別に分割 → 各月のネットワークを構築 → 変遷を比較

  2025年10月のネットワーク:
    品質 ── 改善        配送 ── 遅延

  2025年11月のネットワーク:
    品質 ── 改善        配送 ── 遅延
                        配送 ── 問題  ← 新しい共起が出現!

  2025年12月のネットワーク:
    品質 ── 改善        配送 ── 遅延
    品質 ── 低下 ← 新  配送 ── 問題
                        配送 ── クレーム ← 新しい共起が出現!

→ 12月に品質低下と配送クレームが増加していることがわかる
```

各時間スライスについて、ノード数、エッジ数、モジュラリティが計算されるため、
ネットワークの**複雑さの変化**も追跡できます。

---

## 第5章：自律型AIエージェント（AIが自ら考えて分析する）

### 5.1 AIエージェントとは

#### 通常のAIツールとの違い

通常のAIツール（ChatGPT に質問するなど）は、**質問に答える**だけです。
AIエージェントは、**自ら考え、計画を立て、実行する**能力を持っています。

```
通常のAIツール:
  人間: 「このデータを分析して」
  AI:   「結果はこうです」
  人間: 「次はこれを調べて」    ← 人間が次のステップを指示
  AI:   「結果はこうです」
  人間: 「まとめて」           ← 人間が指示し続ける

AIエージェント:
  人間: 「このデータから有用なインサイトを見つけて」
  AI:   「まずデータの全体像を把握します...」           ← 自分で計画
        「いくつかの仮説を立てました...」               ← 自分で仮説
        「仮説を検証するために追加分析を実行します...」  ← 自分で実行
        「検証の結果、以下の発見がありました...」       ← 自分で検証
        「総合的なインサイトをまとめました」             ← 自分で統合
```

### 5.2 5フェーズ推論ループ

NexusText AI の分析エージェントは、**5つのフェーズ**を循環しながら分析を深めます。

```
    ┌─────────────────────────────────────┐
    │                                       │
    │    ① 観察（Observe）                   │
    │    データ全体を見渡す                    │
    │         │                              │
    │         ▼                              │
    │    ② 仮説（Hypothesize）               │
    │    気づいたことから推測する              │
    │         │                              │
    │         ▼                              │
    │    ③ 探索（Explore）                    │
    │    仮説を裏付けるデータを探す            │
    │         │                              │
    │         ▼                              │
    │    ④ 検証（Verify）                    │
    │    探索結果を統計的に確認する            │
    │         │                              │
    │         ▼                              │
    │    ⑤ 統合（Synthesize）                │
    │    発見をインサイトとしてまとめる         │
    │                                       │
    └─────────────────────────────────────┘
```

#### Phase 1: 観察（Observe）

データの統計情報と特徴をスキャンし、注目すべきポイントを洗い出します。

```
エージェントの思考（例）:

  "データ全体の統計と特徴をスキャンします"

  観察結果:
  ├── 「テキストの平均文字数は87文字で、比較的短いコメントが多い」
  ├── 「全体の35%が50文字以下の短文で、詳細なフィードバックは少数」
  ├── 「感情分布を見ると、Negativeが45%と高い割合を占めている」
  ├── 「12月のデータに集中的な投稿増加が見られる」
  └── 「"配送"関連の単語が全体の22%のテキストに含まれている」
```

#### Phase 2: 仮説（Hypothesize）

観察結果から、検証可能な仮説を生成します。

```
エージェントの思考（例）:

  "観測結果から仮説を生成します"

  仮説:
  ├── 「12月の投稿増加は、年末セール期間の配送遅延に起因する
  │     のではないか？」
  ├── 「Negative評価の多くは配送関連で、商品品質への不満は
  │     少ないのではないか？」
  └── 「短文のコメント（50文字以下）は満足した顧客からのもので、
        不満を持つ顧客ほど詳細に書く傾向があるのではないか？」
```

#### Phase 3: 探索（Explore）

仮説を検証するために、データを深掘りします。

```
エージェントの思考（例）:

  "仮説の検証に必要なデータ探索を実行します"

  探索:
  仮説1について:
  ├── 裏付け: 「12月のテキストの58%に"配送""遅延""届かない"が含まれる」
  ├── 裏付け: 「11月以前の配送関連コメントは15%程度だった」
  └── 反証: 「12月の投稿の一部は商品不良に関するものもある（12%）」
```

#### Phase 4: 検証（Verify）

探索結果を統計的に検証し、仮説の信頼度を算出します。

```
エージェントの思考（例）:

  "探索結果を統計的に検証します"

  検証結果:
  ├── 仮説1: 信頼度 0.82（裏付け多数、反証は限定的）
  ├── 仮説2: 信頼度 0.75（概ね正しいが例外あり）
  └── 仮説3: 信頼度 0.61（傾向はあるが統計的有意性は弱い）
```

#### Phase 5: 統合（Synthesize）

すべての発見をインサイト（洞察）として構造化し、推奨アクションを提示します。

```
エージェントの出力（例）:

  インサイト1:
  ├── タイトル: 「年末セール期の配送遅延が顧客満足度を大幅に低下」
  ├── 説明: 「12月の配送関連クレームが前月比3.8倍に増加。
  │          繁忙期の物流キャパシティ不足が主因と推定される」
  ├── エビデンス: ["配送が2週間経っても届かない", "追跡番号が更新されない", ...]
  ├── Groundingスコア: 0.85
  └── 推奨アクション:
      ├── 「繁忙期の物流パートナー増強を検討」
      ├── 「配送状況のリアルタイム通知機能を導入」
      └── 「配送遅延時の自動お詫びメール対応」
```

### 5.3 HITL（Human-in-the-Loop）制御

#### なぜ人間の監視が必要か

AIは強力ですが、常に正しい判断をするとは限りません。
特に、ビジネス上重要な意思決定に関わる分析では、**人間の監視と承認**が不可欠です。

NexusText AI では、3つのHITLモードを提供しています。

#### Full Auto（フルオート）モード

```
┌─────────┐     ┌─────────┐     ┌─────────┐     ┌─────────┐     ┌─────────┐
│  観察    │ ──→ │  仮説    │ ──→ │  探索    │ ──→ │  検証    │ ──→ │  統合    │
└─────────┘     └─────────┘     └─────────┘     └─────────┘     └─────────┘
                    ↑
                  承認不要（AIが全自動で実行）

用途: 探索的な初期分析、大量データの一次スクリーニング
リスク: AIの判断ミスに気づきにくい
```

#### Semi-Auto（セミオート）モード [デフォルト]

```
┌─────────┐     ┌─────────┐     ┌─────────┐     ┌─────────┐     ┌─────────┐
│  観察    │ ──→ │  仮説    │ ─?─ │  探索    │ ──→ │  検証    │ ──→ │  統合    │
└─────────┘     └─────────┘  │   └─────────┘     └─────────┘     └─────────┘
                              │
                        ┌─────┴─────┐
                        │ 人間が承認  │  ← 仮説の段階で人間がチェック
                        │ or 修正    │
                        └───────────┘

用途: 通常のビジネス分析（バランスが良い）
利点: 重要な判断ポイントで人間が介入できる
```

#### Guided（ガイド）モード

```
┌─────────┐     ┌─────────┐     ┌─────────┐     ┌─────────┐     ┌─────────┐
│  観察    │ ─?─ │  仮説    │ ─?─ │  探索    │ ─?─ │  検証    │ ─?─ │  統合    │
└─────────┘  │   └─────────┘  │   └─────────┘  │   └─────────┘  │   └─────────┘
              │                │                │                │
        ┌─────┴──┐      ┌─────┴──┐       ┌─────┴──┐      ┌─────┴──┐
        │承認/修正│      │承認/修正│       │承認/修正│      │承認/修正│
        └────────┘      └────────┘       └────────┘      └────────┘

用途: 監査・コンプライアンス等、高い正確性が求められる場面
利点: 各ステップで人間が確認・修正できる
注意: 人間の作業負荷が高い
```

### 5.4 Groundingスコア

#### AIのハルシネーション（幻覚）とは

AIは時として、**データに基づかない内容をもっともらしく生成**してしまうことがあります。
これを「ハルシネーション（幻覚）」と呼びます。

```
ハルシネーションの例:

  元データ: お客様の声100件（配送と品質に関する苦情が中心）

  AIの出力（ハルシネーション含む）:
    "顧客の30%が価格に不満を持っている"
    → 実際のデータには価格に関する苦情はほとんどない!
    → AIが一般的な知識から「ありそうなこと」を作り出してしまった
```

#### Groundingスコアの仕組み

NexusText AI では、**Groundingスコア**を計算してハルシネーションを検知します。

```
計算の流れ:

  1. AIが生成した文（主張）をEmbeddingに変換
  2. 元のソースデータをEmbeddingに変換
  3. 主張とソースデータのコサイン類似度を計算
  4. 最も高い類似度をGroundingスコアとする

       AIの主張              ソースデータ
     「配送遅延が主因」    [元テキスト1, 元テキスト2, ..., 元テキスト50]
          │                        │
          ▼                        ▼
      Embedding               Embeddings
          │                        │
          └──── コサイン類似度 ─────┘
                     │
                     ▼
              Groundingスコア = 0.85
              （ソースデータに強い根拠あり）
```

#### スコアの解釈

```
Groundingスコアの目安:

  0.9 ~ 1.0  ← ソースデータに直接的な根拠がある（非常に信頼性が高い）
  0.7 ~ 0.9  ← ソースデータに間接的な根拠がある（信頼性が高い）
  0.5 ~ 0.7  ← 根拠が弱い（注意して確認が必要）
  0.0 ~ 0.5  ← 根拠がほとんどない（ハルシネーションの可能性が高い）
```

NexusText AI では、Groundingスコアが0.5未満のインサイトには警告を表示し、
ユーザーに確認を促します。

---

## 第6章：レポート生成（分析結果をまとめる）

### 6.1 テンプレートの種類と用途

NexusText AI では、4つのプリセットテンプレートと1つのカスタムテンプレートが用意されています。

```
┌───────────────────────────────────────────────────────────────┐
│                   レポートテンプレート一覧                       │
├──────────┬────────────────────────────────────────────────────┤
│ テンプレート │ 用途・セクション構成                                │
├──────────┼────────────────────────────────────────────────────┤
│          │ ① エグゼクティブサマリー                              │
│ VoC      │ ② 感情トレンド分析                                   │
│ (顧客の声)│ ③ クラスター分析結果                                 │
│          │ ④ 主要テーマ別詳細                                   │
│          │ ⑤ 改善提案                                          │
├──────────┼────────────────────────────────────────────────────┤
│          │ ① 分析概要                                          │
│ Audit    │ ② 主要発見事項                                      │
│ (監査)    │ ③ リスク評価                                        │
│          │ ④ 統制上の懸念点                                     │
│          │ ⑤ 推奨事項                                          │
├──────────┼────────────────────────────────────────────────────┤
│          │ ① 調査概要                                          │
│Compliance│ ② 時系列分析                                        │
│(コンプラ) │ ③ キーワード共起分析                                  │
│          │ ④ リスク分類                                        │
│          │ ⑤ 結論と提言                                        │
├──────────┼────────────────────────────────────────────────────┤
│          │ ① リスク分析概要                                     │
│ Risk     │ ② リスク分類別集計                                   │
│ (リスク)  │ ③ ヒートマップ分析                                   │
│          │ ④ 優先対応事項                                       │
│          │ ⑤ モニタリング計画                                   │
├──────────┼────────────────────────────────────────────────────┤
│ Custom   │ ユーザーが自由にプロンプトを指定して                     │
│ (カスタム) │ セクション構成を自動生成（5-7セクション）               │
└──────────┴────────────────────────────────────────────────────┘
```

#### テンプレートの選び方

```
目的は何ですか？
    │
    ├── お客様の声を分析したい
    │   → VoC テンプレート
    │
    ├── 内部監査の所見を整理したい
    │   → Audit テンプレート
    │
    ├── 法令遵守の状況を報告したい
    │   → Compliance テンプレート
    │
    ├── リスクを評価・管理したい
    │   → Risk テンプレート
    │
    └── 上記に当てはまらない独自の分析
        → Custom テンプレート
```

### 6.2 LLMによるセクション生成

各セクションは、分析データを基にLLMが自動生成します。

```
セクション生成の流れ:

  分析データ（クラスタリング結果、感情分析結果、共起ネットワーク結果）
      │
      ▼
  ┌─────────────────────────────────────────────┐
  │  LLM（Claude Opus）                           │
  │                                               │
  │  要件:                                         │
  │  ├── ビジネスパーソン向けの明確な文章           │
  │  ├── データに基づく具体的な記述                 │
  │  ├── 各記述にエビデンス（根拠）の参照を含める   │
  │  └── 200-400字程度                             │
  └─────────────────────────────────────────────┘
      │
      ▼
  セクション出力:
  ├── タイトル（セクション名）
  ├── 本文（200-400字のビジネス文書）
  └── エビデンス参照（根拠となる元データのリスト）
```

#### エビデンスリンクの自動付与

レポートの各記述には、元データへの**エビデンスリンク**が自動的に付与されます。
これにより、レポートの読者は「この結論の根拠は何か」をいつでも確認できます。

```
レポートの例:

  【エグゼクティブサマリー】

  2025年第4四半期において、顧客フィードバックの感情分析では
  Negative評価が前期比15ポイント増加しました [根拠1][根拠2]。
  主な要因は配送遅延で、特に12月に集中しています [根拠3]。
  クラスター分析の結果、「配送遅延への不満」が最大のクラスター
  （全体の28%）を形成しています [根拠4]。

  [根拠1] 感情分析結果: Negative 45% (前期30%)
  [根拠2] 時系列データ: 10月28% → 11月35% → 12月52%
  [根拠3] クラスター3「配送遅延」: 842件中の72%が12月
  [根拠4] クラスター分析結果: シルエットスコア 0.65
```

### 6.3 出力形式の選び方

NexusText AI では、4つの出力形式に対応しています。

```
┌──────────┬──────────────────────────┬────────────────────────────┐
│ 形式      │ 特徴                      │ おすすめの場面              │
├──────────┼──────────────────────────┼────────────────────────────┤
│ PPTX     │ スライド形式               │ 経営会議でのプレゼン        │
│ (パワポ)  │ 各セクションが1スライド    │ 幹部向け報告                │
├──────────┼──────────────────────────┼────────────────────────────┤
│ PDF      │ 印刷に最適                 │ 正式な報告書                │
│          │ レイアウトが固定           │ 監査報告、コンプライアンス  │
├──────────┼──────────────────────────┼────────────────────────────┤
│ DOCX     │ 編集可能                   │ ドラフト版の共有            │
│ (ワード)  │ エビデンスが箇条書き      │ チームでのレビュー          │
├──────────┼──────────────────────────┼────────────────────────────┤
│ Excel    │ 表形式                    │ データの二次加工             │
│          │ セクション/内容/根拠の列   │ 他ツールとの連携            │
└──────────┴──────────────────────────┴────────────────────────────┘

選び方のフローチャート:

  誰に見せますか？
      │
      ├── 経営層・幹部 → PPTX（視覚的にインパクトがある）
      │
      ├── 監査法人・規制当局 → PDF（改ざん防止、正式文書）
      │
      ├── チームメンバー → DOCX（編集・コメント追加が可能）
      │
      └── 自分自身・データ分析チーム → Excel（データの再利用が容易）
```

---

## 用語集

テキストマイニングとNexusText AI で使用される用語を、日本語の五十音順で整理しました。

| 用語（日本語） | 用語（英語） | 説明 |
|---|---|---|
| アウトライヤー | Outlier | どのクラスターにも属さない異常なデータ点。新しいトピックや入力ミスの可能性を示す |
| アルゴリズム | Algorithm | 問題を解決するための手順や計算方法のこと。「レシピ」のようなもの |
| ウィンドウサイズ | Window Size | 共起分析で「何単語以内」を共起とみなすかの範囲設定 |
| エージェント | Agent | 自ら考え、計画し、実行するAIプログラム。単なるツールではなく自律的に動作する |
| エッジ | Edge | ネットワークグラフにおける線（つながり）。2つのノードの関係を表す |
| エビデンス | Evidence | 分析結果の根拠となるデータや事実。AIの出力を信頼するための裏付け |
| エンベディング | Embedding | テキストを数値ベクトルに変換すること。意味の近いテキストは近いベクトルになる |
| ガウス混合モデル | Gaussian Mixture Model (GMM) | 確率的なクラスタリング手法。各データ点が各クラスターに属する確率を計算する |
| カスタム軸 | Custom Axis | ユーザーが独自に定義する感情分析の分類基準 |
| 感情分析 | Sentiment Analysis | テキストに含まれる感情や意図を自動的に判定する技術 |
| 基数中心性 | Degree Centrality | ネットワーク内で、あるノードがどれだけ多くのノードとつながっているかの指標 |
| 共起 | Co-occurrence | 2つの単語が同じ文脈（近い位置）に一緒に出現すること |
| 共起行列 | Co-occurrence Matrix | 全単語ペアの共起回数をまとめた表 |
| クラスター | Cluster | 類似したデータのグループ。テキスト分析では似た内容の文章群を指す |
| クラスタリング | Clustering | データを類似性に基づいてグループに分ける分析手法 |
| グラウンディング | Grounding | AIの出力が元データに基づいているかを検証すること |
| グラウンディングスコア | Grounding Score | AIの主張がソースデータにどの程度根拠を持つかの数値指標（0.0-1.0） |
| コサイン類似度 | Cosine Similarity | 2つのベクトルの方向の類似度を測る指標。1.0に近いほど似ている |
| コミュニティ | Community | ネットワーク内で互いに密接につながっているノードの集団 |
| コミュニティ検出 | Community Detection | ネットワーク内のコミュニティを自動的に見つけるアルゴリズム |
| コンプライアンス | Compliance | 法令や社内規則を遵守すること |
| サーキットブレーカー | Circuit Breaker | API呼び出しの連続失敗を検知し、自動的にフォールバックする仕組み |
| 次元削減 | Dimensionality Reduction | 高次元データを低次元（2Dや3D）に圧縮して可視化可能にする技術 |
| シルエットスコア | Silhouette Score | クラスタリングの品質を-1.0から1.0で評価する指標。高いほど良い分離 |
| ストップワード | Stopword | 文章中に頻出するが分析上の意味が薄い単語（「は」「の」「が」など） |
| スパイク | Spike | 時系列データにおける急激な変化。通常の変動範囲を超えた異常値 |
| スパイク検知 | Spike Detection | 移動平均と標準偏差を使って時系列データの急激な変化を自動検出する手法 |
| セクション | Section | レポートを構成する各章・各パートのこと |
| セントロイド | Centroid | クラスターの中心点。そのクラスターに属するすべてのデータの平均位置 |
| ソースデータ | Source Data | 分析の元となるデータ。エビデンスの根拠となる |
| テキストマイニング | Text Mining | 大量のテキストデータから有用な情報やパターンを自動的に抽出する技術 |
| テンプレート | Template | レポートの構成（セクションの種類と順序）を定めたひな型 |
| トークナイゼーション | Tokenization | 文章を単語やサブワード単位に分割する処理 |
| トークン | Token | トークナイゼーションで分割された最小単位（単語やサブワード） |
| ノード | Node | ネットワークグラフにおける点。共起分析では各単語に対応する |
| ハブ | Hub | ネットワーク内で多くのノードとつながっている中心的なノード |
| ハルシネーション | Hallucination | AIがデータに基づかない内容をもっともらしく生成してしまう現象 |
| バッチ処理 | Batch Processing | データを一定数（例: 10件）ずつまとめて処理すること。効率化のため |
| パラメータ | Parameter | アルゴリズムの動作を調整するための設定値 |
| ヒートマップ | Heatmap | 数値の大きさを色の濃淡で表現した図表 |
| フォールバック | Fallback | 主要な処理が失敗した場合に自動的に切り替わる代替手段 |
| ベクトル | Vector | 複数の数値の並び。テキストの意味を数値で表現する手段 |
| 媒介中心性 | Betweenness Centrality | ネットワーク内で異なるグループを橋渡しする度合いを表す指標 |
| マルチラベル | Multi-label | 1つのテキストに複数のラベル（分類）を付与すること |
| モジュラリティ | Modularity | コミュニティ分割の品質を0.0-1.0で評価する指標。高いほど明確な構造 |
| ユーマップ | UMAP | 高次元データを低次元に圧縮する次元削減手法。近いデータは近くに配置される |
| ラベリング | Labeling | クラスターやテキストに名前や分類を付与すること |
| ルーヴァン法 | Louvain Method | ネットワーク内のコミュニティを高速に検出するアルゴリズム |
| LLM | Large Language Model | 大規模言語モデル。大量のテキストで訓練されたAIモデル |
| HITL | Human-in-the-Loop | AIの判断に人間が介入・監視する仕組み |
| HDBSCAN | - | 密度ベースのクラスタリング手法。クラスター数の自動決定と外れ値検出が可能 |
| K-Means | - | 最も基本的なクラスタリング手法。k個のグループにデータを分割する |
| MeCab | - | 日本語の形態素解析エンジン。文章を単語に分割する |
| NFKC | - | Unicode正規化の一方式。全角半角の統一や互換文字の正規化を行う |
| PII | Personally Identifiable Information | 個人情報。氏名、住所、電話番号など個人を特定できる情報 |
| Sentence-BERT | - | 文章レベルの意味をベクトルに変換する深層学習モデル |

---

## 付録：NexusText AI v7.0 システム構成

参考として、NexusText AI v7.0 の主要コンポーネントの関係を示します。

```
┌──────────────────────────────────────────────────────────────────────┐
│                        NexusText AI v7.0                              │
├──────────────────────────────────────────────────────────────────────┤
│                                                                       │
│  ┌──────────────┐    ┌──────────────────────┐                        │
│  │ データ入力      │    │  LLMオーケストレーター  │                       │
│  │ CSV/Excel      │    │                        │                       │
│  │ エンコード自動検出│   │  Claude Opus ────┐    │                       │
│  └──────┬───────┘    │  Claude Sonnet ──┤    │                       │
│          │             │  GPT-5 ─────────┤    │                       │
│          ▼             │  Gemini ────────┤    │                       │
│  ┌──────────────┐    │  Llama（ローカル）─┘    │                       │
│  │ 前処理          │    │                        │                       │
│  │ HTML除去        │    │  サーキットブレーカー    │                       │
│  │ NFKC正規化      │    │  自動フォールバック      │                       │
│  │ トークナイゼーション│  └──────────┬───────────┘                       │
│  │ ストップワード除去 │              │                                    │
│  │ Embedding生成   │              │                                    │
│  └──────┬───────┘              │                                    │
│          │                        │                                    │
│          ▼                        ▼                                    │
│  ┌─────────────────────────────────────────┐                          │
│  │              分析サービス群                  │                         │
│  │                                             │                         │
│  │  ┌─────────────┐  ┌─────────────────┐     │                         │
│  │  │ クラスタリング  │  │ 感情分析          │     │                        │
│  │  │ K-Means       │  │ Basic/Business/Risk│    │                        │
│  │  │ HDBSCAN       │  │ Custom/MultiLabel  │    │                        │
│  │  │ GMM           │  │ 時系列/スパイク    │     │                        │
│  │  └─────────────┘  └─────────────────┘     │                         │
│  │                                             │                         │
│  │  ┌─────────────┐  ┌─────────────────┐     │                         │
│  │  │ 共起ネットワーク│  │ 自律型エージェント  │    │                        │
│  │  │ Louvain法     │  │ 5フェーズ推論      │     │                        │
│  │  │ 中心性指標     │  │ HITL制御          │     │                        │
│  │  │ 時間スライス   │  │ Groundingスコア    │     │                        │
│  │  └─────────────┘  └─────────────────┘     │                         │
│  └─────────────────────────────────────────┘                          │
│          │                                                              │
│          ▼                                                              │
│  ┌──────────────┐                                                     │
│  │ レポート生成     │                                                    │
│  │ VoC/Audit/      │                                                    │
│  │ Compliance/Risk │                                                    │
│  │ → PPTX/PDF/     │                                                    │
│  │   DOCX/Excel    │                                                    │
│  └──────────────┘                                                     │
│                                                                       │
└──────────────────────────────────────────────────────────────────────┘
```

---

> **本ガイドについて**
>
> このガイドは NexusText AI v7.0 の分析手法を初心者向けに解説したものです。
> 各手法の理論的な背景を簡略化して説明しているため、学術的な厳密さよりも
> 実用的なわかりやすさを優先しています。
>
> より詳細な技術情報については、各ライブラリの公式ドキュメントを参照してください。
> - scikit-learn（K-Means, GMM, シルエットスコア）
> - hdbscan（HDBSCAN）
> - umap-learn（UMAP）
> - sentence-transformers（Sentence-BERT）
> - python-louvain（Louvain法）
> - NetworkX（ネットワーク分析）
> - MeCab / fugashi（日本語形態素解析）
